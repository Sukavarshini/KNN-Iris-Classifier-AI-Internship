# KNN-Iris-Classifier-AI-Internship
# ğŸŒ¼ KNN Iris Classifier â€“ AI & ML Internship Task 6

This project implements the **K-Nearest Neighbors (KNN)** classification algorithm using the **Iris dataset**, submitted as part of **Task 6** for the AI & ML Internship. It includes accuracy comparison across K values, confusion matrix evaluation, and 2D decision boundary visualization.

---

## ğŸ¯ Objective

- Understand the working of the KNN algorithm for multi-class classification.
- Analyze model performance for different K values.
- Visualize decision boundaries and accuracy trends.
- Apply feature scaling for distance-based algorithms.

---

## ğŸ§  Concepts Covered

- Instance-based Learning
- Feature Normalization (StandardScaler)
- K Value Selection
- Accuracy & Confusion Matrix
- Decision Boundary Visualization (2D)
- Euclidean Distance Metric

---

## ğŸ”§ Technologies Used

- Python 3
- Scikit-learn
- Matplotlib
- NumPy
- Git & GitHub

---

## ğŸ“ Project Structure

```
KNN-Iris-Classifier-AI-Internship/
â”œâ”€â”€ knn_classifier.py
â”œâ”€â”€ accuracy_plot.png
â”œâ”€â”€ decision_boundary_plot.png
â””â”€â”€ README.md
```

---

## ğŸ“Š Results

- âœ… **Best K value**: `3`
- âœ… **Accuracy**: `100%` on test dataset
- âœ… **Confusion Matrix**:
  ```
  [[19  0  0]
   [ 0 13  0]
   [ 0  0 13]]
  ```

- ğŸ“ˆ Accuracy plot: saved as `accuracy_plot.png`
- ğŸŒˆ Decision boundary: saved as `decision_boundary_plot.png`

---

## ğŸ“š Dataset

- **Iris Dataset**
  - Features: Sepal length, Sepal width, Petal length, Petal width
  - Classes: Setosa, Versicolor, Virginica
  - Source: [https://www.kaggle.com/datasets/uciml/iris](https://www.kaggle.com/datasets/uciml/iris)

---

## ğŸ“¤ Submission Info

- ğŸ”– **Task**: Task 6 â€“ KNN Classifier
- ğŸ‘©â€ğŸ’» **Intern**: Sukavarshini
- ğŸ“¬ **Email**: varshini8754@gmail.com
- ğŸ”— **GitHub Repo**: [https://github.com/Sukavarshini/KNN-Iris-Classifier-AI-Internship](https://github.com/Sukavarshini/KNN-Iris-Classifier-AI-Internship)

---

## ğŸ’¬ Key Questions Answered

**Q: Why is normalization important in KNN?**  
KNN uses distance to determine neighbors. Without normalization, features with large scales dominate.

**Q: How do you choose the right K?**  
Try multiple K values (e.g., 1 to 10), plot accuracy, and pick the one with the best performance.

**Q: Is KNN sensitive to noise?**  
Yes. A small K value may be sensitive to outliers and noise.

**Q: Does KNN handle multi-class problems?**  
Yes. It predicts the majority class among the K nearest neighbors, even for more than 2 classes.

---

## âœ… Conclusion

This task helped reinforce the importance of data preprocessing in distance-based algorithms and provided visual insights into how KNN performs on simple datasets like Iris.



---
